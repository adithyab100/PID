{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from get_data import get_dataloader # noqa\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from unimodals.common_models import MLP # noqa\n",
    "from training_structures.unimodal import train, test # noqa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_density(data, bandwidth=1.0, kernel='gaussian'):\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth)\n",
    "    kde.fit(data)\n",
    "    return kde\n",
    "\n",
    "def get_log_density(kde_model, input_data):\n",
    "    \"\"\"\n",
    "    Get the probability density of an arbitrary 64-dimensional input.\n",
    "\n",
    "    Parameters:\n",
    "    kde_model: Trained KernelDensity model.\n",
    "    input_data (np.ndarray): A single 64-dimensional input, or multiple inputs (n_samples, 64).\n",
    "\n",
    "    Returns:\n",
    "    density (np.ndarray): The estimated probability density for the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure input data has the correct number of dimensions\n",
    "    if input_data.ndim == 1:\n",
    "        input_data = input_data.reshape(1, -1)\n",
    "\n",
    "    # Estimate log density for the input\n",
    "    log_density = kde_model.score_samples(input_data)\n",
    "\n",
    "    # # Convert log density to regular density\n",
    "    # density = np.exp(log_density)\n",
    "\n",
    "    return log_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -3.4645936982209498, 1: -3.4645936982209498, 2: -3.4645936982209498, 3: -3.4645936982209498, 4: -3.4668794135018053, 5: -3.4645936982209498, 6: -3.4668794135018053, 7: -3.4668794135018053, 8: -3.4668794135018053, 9: -3.4668794135018053, 10: -3.4668794135018053, 11: -3.4645936982209498, 12: -3.4668794135018053, 13: -3.4668794135018053, 14: -3.4645936982209498, 15: -3.4645936982209498, 16: -3.4645936982209498, 17: -3.4668794135018053, 18: -3.4645936982209498, 19: -3.4668794135018053, 20: -3.4668794135018053, 21: -3.4668794135018053, 22: -3.4668794135018053, 23: -3.4645936982209498, 24: -3.4645936982209498, 25: -3.4645936982209498, 26: -3.4645936982209498, 27: -3.4668794135018053, 28: -3.4668794135018053, 29: -3.4645936982209498, 30: -3.4645936982209498, 31: -3.4668794135018053}\n"
     ]
    }
   ],
   "source": [
    "kdes = {}\n",
    "label_pdf = {}\n",
    "setting = 'redundancy'\n",
    "with open(f'experiments/DATA_{setting}.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)['train']\n",
    "    for modality in {'0', '1'}:\n",
    "        data_mod = data[modality]\n",
    "        kde = estimate_density(data_mod)\n",
    "        kdes[modality] = kde\n",
    "    \n",
    "\n",
    "    label_count = np.bincount(data['label'].flatten())\n",
    "    # convert label_count to log distribution\n",
    "    label_probs = np.log(label_count / np.sum(label_count))\n",
    "    label_pdf = {i: label_probs[i] for i in range(len(label_probs))}\n",
    "\n",
    "print(label_pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': MLP(\n",
      "  (fc): Linear(in_features=64, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=600, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  (lklu): LeakyReLU(negative_slope=0.2)\n",
      "), '1': MLP(\n",
      "  (fc): Linear(in_features=64, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=600, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  (lklu): LeakyReLU(negative_slope=0.2)\n",
      ")} {'0': MLP(\n",
      "  (fc): Linear(in_features=600, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  (lklu): LeakyReLU(negative_slope=0.2)\n",
      "), '1': MLP(\n",
      "  (fc): Linear(in_features=600, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=32, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.1, inplace=False)\n",
      "  (lklu): LeakyReLU(negative_slope=0.2)\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "encoders = {}\n",
    "heads = {}\n",
    "\n",
    "saved_model = f'experiments/{setting}/{setting}_unimodal'\n",
    "\n",
    "for modality in {'0', '1'}:\n",
    "    saved_encoder = saved_model + '{}_encoder.pt'.format(modality)\n",
    "    saved_head = saved_model + '{}_head.pt'.format(modality)\n",
    "    encoders[modality] = torch.load(saved_encoder).to(device)\n",
    "    heads[modality] = torch.load(saved_head).to(device)\n",
    "\n",
    "print(encoders, heads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7683794991547899\n",
      "100 0.8228630903860259\n",
      "200 0.9016114178574897\n",
      "300 0.8366690431806286\n",
      "400 0.8116699778711653\n",
      "500 0.8160218091528525\n",
      "600 0.8376742147470094\n",
      "700 0.8417209950635236\n",
      "800 0.849901790348339\n",
      "900 0.8477636928517439\n",
      "1000 0.8577006669988861\n",
      "1100 0.8394662120899146\n",
      "1200 0.8385568759383508\n",
      "1300 0.817681023963302\n",
      "1400 0.8204898410844053\n",
      "1500 0.8036135105683314\n",
      "1600 0.8071484738401669\n",
      "1700 0.7992397272156517\n",
      "1800 0.7999230824637392\n",
      "1900 0.8085904178745592\n",
      "2000 0.8100914645130783\n",
      "2100 0.8054932015250031\n",
      "2200 0.8050300144242835\n",
      "2300 0.8022557010103539\n",
      "2400 0.8167577640181217\n",
      "2500 0.8225909768265989\n",
      "2600 0.8200791220069193\n",
      "2700 0.825026299731818\n",
      "2800 0.828528653761138\n",
      "2900 0.8281742129631645\n",
      "0.8299158114354036\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(f'experiments/DATA_{setting}.pickle', 'rb') as f:\n",
    "    r_tot = 0\n",
    "    data = pickle.load(f)['test']\n",
    "    #compute mutual information between data['0'] and data['label']\n",
    "    \n",
    "    for i in range(len(data['0'])):\n",
    "        #print(get_log_density(kdes['0'], data['0'][i]), get_log_density(kdes['1'], data['1'][i]))\n",
    "        r_p = -max(get_log_density(kdes['0'], data['0'][i]), get_log_density(kdes['1'], data['1'][i]))\n",
    "\n",
    "        model = nn.Sequential(encoders['0'], heads['0'])\n",
    "        model = model.to(device)\n",
    "        #print(data['label'][i].shape)\n",
    "        out = model(torch.from_numpy(data['0'][i]).float().to(device))\n",
    "        #print(torch.log(softmax(out)[data['label'][i][0]]).item(),label_pdf[data['label'][i][0]],get_log_density(kdes['0'], data['0'][i]))\n",
    "        r_m_0 = -torch.log(softmax(out)[data['label'][i][0]]).item() + label_pdf[data['label'][i][0]] - get_log_density(kdes['0'], data['0'][i])\n",
    "        \n",
    "        model = nn.Sequential(encoders['1'], heads['1'])\n",
    "        model = model.to(device)\n",
    "        out = model(torch.from_numpy(data['1'][i]).float().to(device))\n",
    "        r_m_1 = -torch.log(softmax(out)[data['label'][i][0]]).item() + label_pdf[data['label'][i][0]] - get_log_density(kdes['1'], data['1'][i])\n",
    "\n",
    "        r_m = min(r_m_0, r_m_1)\n",
    "\n",
    "        #print(r_p[0], r_m[0], r_p[0] - r_m[0])\n",
    "        r_tot += r_p[0] - r_m[0]\n",
    "        if i % 100 == 0:\n",
    "            print(i,r_tot/(i+1))\n",
    "    print(r_tot/len(data['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adithya/opt/anaconda3/envs/pid/lib/python3.9/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 0.00000000e+00 5.28368569e-03 2.79933609e-03\n",
      " 0.00000000e+00 0.00000000e+00 2.02423294e-03 0.00000000e+00\n",
      " 7.18903370e-03 2.34222098e-03 7.20770169e-04 3.49873650e-03\n",
      " 0.00000000e+00 9.00163278e-03 0.00000000e+00 1.75211585e-03\n",
      " 0.00000000e+00 2.70416117e-03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.79431900e-03 3.65050921e-03\n",
      " 0.00000000e+00 7.38596608e-03 1.76865524e-03 3.33311401e-04\n",
      " 0.00000000e+00 8.51585707e-03 8.30891574e-03 0.00000000e+00\n",
      " 7.30608258e-01 3.92350783e-01 5.26249432e-01 3.63501193e-01\n",
      " 5.30752023e-01 6.62302108e-01 3.72255246e-01 5.46606729e-01\n",
      " 4.02091675e-01 4.88761401e-01 4.59198093e-01 3.55607873e-01\n",
      " 5.15717008e-01 2.50008520e-01 4.68811705e-01 4.42193052e-01\n",
      " 5.01751442e-01 6.96141277e-01 4.73771180e-01 4.30744400e-01\n",
      " 6.15738100e-01 4.31457076e-01 5.53663045e-01 2.67632752e-01\n",
      " 5.40123196e-01 6.84772192e-01 7.16779286e-01 5.74439618e-01\n",
      " 2.68479110e-01 5.92062233e-01 6.35010892e-01 5.76291530e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "print(mutual_info_classif(data['0'], data['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pw_redundancy(setting):\n",
    "    kdes = {}\n",
    "    label_pdf = {}\n",
    "    with open(f'experiments/DATA_{setting}.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)['train']\n",
    "        for modality in {'0', '1'}:\n",
    "            data_mod = data[modality]\n",
    "            kde = estimate_density(data_mod)\n",
    "            kdes[modality] = kde\n",
    "        \n",
    "\n",
    "        label_count = np.bincount(data['label'].flatten())\n",
    "        # convert label_count to log distribution\n",
    "        label_probs = np.log(label_count / np.sum(label_count))\n",
    "        label_pdf = {i: label_probs[i] for i in range(len(label_probs))}\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    encoders = {}\n",
    "    heads = {}\n",
    "\n",
    "    saved_model = f'experiments/{setting}/{setting}_unimodal'\n",
    "\n",
    "    for modality in {'0', '1'}:\n",
    "        saved_encoder = saved_model + '{}_encoder.pt'.format(modality)\n",
    "        saved_head = saved_model + '{}_head.pt'.format(modality)\n",
    "        encoders[modality] = torch.load(saved_encoder).to(device)\n",
    "        heads[modality] = torch.load(saved_head).to(device)\n",
    "\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    with open(f'experiments/DATA_{setting}.pickle', 'rb') as f:\n",
    "        r_tot = 0\n",
    "        data = pickle.load(f)['test']\n",
    "        #compute mutual information between data['0'] and data['label']\n",
    "        \n",
    "        for i in range(len(data['0'])):\n",
    "            #print(get_log_density(kdes['0'], data['0'][i]), get_log_density(kdes['1'], data['1'][i]))\n",
    "            r_p = -max(get_log_density(kdes['0'], data['0'][i]), get_log_density(kdes['1'], data['1'][i]))\n",
    "\n",
    "            model = nn.Sequential(encoders['0'], heads['0'])\n",
    "            model = model.to(device)\n",
    "            #print(data['label'][i].shape)\n",
    "            out = model(torch.from_numpy(data['0'][i]).float().to(device))\n",
    "            #print(torch.log(softmax(out)[data['label'][i][0]]).item(),label_pdf[data['label'][i][0]],get_log_density(kdes['0'], data['0'][i]))\n",
    "            r_m_0 = -torch.log(softmax(out)[data['label'][i][0]]).item() + label_pdf[data['label'][i][0]] - get_log_density(kdes['0'], data['0'][i])\n",
    "            \n",
    "            model = nn.Sequential(encoders['1'], heads['1'])\n",
    "            model = model.to(device)\n",
    "            out = model(torch.from_numpy(data['1'][i]).float().to(device))\n",
    "            r_m_1 = -torch.log(softmax(out)[data['label'][i][0]]).item() + label_pdf[data['label'][i][0]] - get_log_density(kdes['1'], data['1'][i])\n",
    "\n",
    "            r_m = min(r_m_0, r_m_1)\n",
    "\n",
    "            #print(r_p[0], r_m[0], r_p[0] - r_m[0])\n",
    "            r_tot += r_p[0] - r_m[0]\n",
    "            # if i % 100 == 0:\n",
    "            #     print(i,r_tot/(i+1))\n",
    "        return r_tot/len(data['0'])/np.log(2) * 0.2\n",
    "        # print(r_tot/len(data['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23946308510263936"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_pw_redundancy('redundancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14801657174130795"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_pw_redundancy('uniqueness0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1708824815495391"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_pw_redundancy('uniqueness1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05331318649827324"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_pw_redundancy('synergy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Mutual Information (joint) between X and y: [0.00000000e+00 0.00000000e+00 5.28368569e-03 2.79933609e-03\n",
      " 0.00000000e+00 0.00000000e+00 2.02423294e-03 0.00000000e+00\n",
      " 7.18903370e-03 2.34222098e-03 7.20770169e-04 3.49873650e-03\n",
      " 0.00000000e+00 9.00163278e-03 0.00000000e+00 1.75211585e-03\n",
      " 0.00000000e+00 2.70416117e-03 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.79431900e-03 3.65050921e-03\n",
      " 0.00000000e+00 7.38596608e-03 1.76865524e-03 3.33311401e-04\n",
      " 0.00000000e+00 8.51585707e-03 8.30891574e-03 0.00000000e+00\n",
      " 7.30608258e-01 3.92350783e-01 5.26249432e-01 3.63501193e-01\n",
      " 5.30752023e-01 6.62302108e-01 3.72255246e-01 5.46606729e-01\n",
      " 4.02091675e-01 4.88761401e-01 4.59198093e-01 3.55607873e-01\n",
      " 5.15717008e-01 2.50008520e-01 4.68811705e-01 4.42193052e-01\n",
      " 5.01751442e-01 6.96141277e-01 4.73771180e-01 4.30744400e-01\n",
      " 6.15738100e-01 4.31457076e-01 5.53663045e-01 2.67632752e-01\n",
      " 5.40123196e-01 6.84772192e-01 7.16779286e-01 5.74439618e-01\n",
      " 2.68479110e-01 5.92062233e-01 6.35010892e-01 5.76291530e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "with open(f'experiments/DATA_redundancy.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)['train']\n",
    "    X = data['0']\n",
    "    y = data['label'].flatten()\n",
    "\n",
    "    # Discretize the continuous feature data into bins\n",
    "    discretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n",
    "    X_binned = discretizer.fit_transform(X)\n",
    "\n",
    "    # Convert multidimensional features to tuples (joint representation)\n",
    "    X_tuples = np.apply_along_axis(lambda row: tuple(row), 1, X_binned)\n",
    "\n",
    "    # Compute total mutual information between joint X (as tuples) and y\n",
    "    total_mi_joint = mutual_info_classif(X, y)\n",
    "\n",
    "    print(f\"Total Mutual Information (joint) between X and y: {total_mi_joint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
